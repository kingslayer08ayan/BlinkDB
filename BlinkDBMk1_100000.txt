ayan@DESKTOP-3JNEF5K:/mnt/e/KGPSem2/Design_Lab/BlinkDB$ redis-benchmark -h 127.0.0.1 -p 6379 -n 100000 -t set,get
====== SET ======                                                   
  100000 requests completed in 2.62 seconds
  50 parallel clients
  3 bytes payload
  keep alive: 1
  host configuration "save": 3600 1 300 100 60 10000
  host configuration "appendonly": no
  multi-thread: no

Latency by percentile distribution:
0.000% <= 0.199 milliseconds (cumulative count 2)
50.000% <= 0.631 milliseconds (cumulative count 50365)
75.000% <= 0.775 milliseconds (cumulative count 75223)
87.500% <= 1.055 milliseconds (cumulative count 87803)
93.750% <= 1.263 milliseconds (cumulative count 93782)
96.875% <= 1.551 milliseconds (cumulative count 96923)
98.438% <= 2.079 milliseconds (cumulative count 98443)
99.219% <= 3.031 milliseconds (cumulative count 99226)
99.609% <= 3.967 milliseconds (cumulative count 99610)
99.805% <= 4.751 milliseconds (cumulative count 99805)
99.902% <= 6.119 milliseconds (cumulative count 99903)
99.951% <= 7.639 milliseconds (cumulative count 99952)
99.976% <= 12.039 milliseconds (cumulative count 99976)
99.988% <= 12.311 milliseconds (cumulative count 99988)
99.994% <= 12.447 milliseconds (cumulative count 99994)
99.997% <= 12.591 milliseconds (cumulative count 99997)
99.998% <= 12.663 milliseconds (cumulative count 99999)
99.999% <= 12.735 milliseconds (cumulative count 100000)
100.000% <= 12.735 milliseconds (cumulative count 100000)

Cumulative distribution of latencies:
0.000% <= 0.103 milliseconds (cumulative count 0)
0.002% <= 0.207 milliseconds (cumulative count 2)
0.094% <= 0.303 milliseconds (cumulative count 94)
1.846% <= 0.407 milliseconds (cumulative count 1846)
11.581% <= 0.503 milliseconds (cumulative count 11581)
42.189% <= 0.607 milliseconds (cumulative count 42189)
67.384% <= 0.703 milliseconds (cumulative count 67384)
77.368% <= 0.807 milliseconds (cumulative count 77368)
82.202% <= 0.903 milliseconds (cumulative count 82202)
85.865% <= 1.007 milliseconds (cumulative count 85865)
90.089% <= 1.103 milliseconds (cumulative count 90089)
92.713% <= 1.207 milliseconds (cumulative count 92713)
94.439% <= 1.303 milliseconds (cumulative count 94439)
95.803% <= 1.407 milliseconds (cumulative count 95803)
96.641% <= 1.503 milliseconds (cumulative count 96641)
97.230% <= 1.607 milliseconds (cumulative count 97230)
97.645% <= 1.703 milliseconds (cumulative count 97645)
97.933% <= 1.807 milliseconds (cumulative count 97933)
98.141% <= 1.903 milliseconds (cumulative count 98141)
98.321% <= 2.007 milliseconds (cumulative count 98321)
98.482% <= 2.103 milliseconds (cumulative count 98482)
99.272% <= 3.103 milliseconds (cumulative count 99272)
99.652% <= 4.103 milliseconds (cumulative count 99652)
99.836% <= 5.103 milliseconds (cumulative count 99836)
99.902% <= 6.103 milliseconds (cumulative count 99902)
99.930% <= 7.103 milliseconds (cumulative count 99930)
99.968% <= 8.103 milliseconds (cumulative count 99968)
99.970% <= 9.103 milliseconds (cumulative count 99970)
99.978% <= 12.103 milliseconds (cumulative count 99978)
100.000% <= 13.103 milliseconds (cumulative count 100000)

Summary:
  throughput summary: 38197.10 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        0.752     0.192     0.631     1.351     2.751    12.735
====== GET ======                                                   
  100000 requests completed in 2.96 seconds
  50 parallel clients
  3 bytes payload
  keep alive: 1
  host configuration "save": 3600 1 300 100 60 10000
  host configuration "appendonly": no
  multi-thread: no

Latency by percentile distribution:
0.000% <= 0.239 milliseconds (cumulative count 1)
50.000% <= 0.655 milliseconds (cumulative count 50381)
75.000% <= 1.031 milliseconds (cumulative count 75906)
87.500% <= 1.239 milliseconds (cumulative count 87598)
93.750% <= 1.695 milliseconds (cumulative count 93771)
96.875% <= 2.575 milliseconds (cumulative count 96884)
98.438% <= 3.183 milliseconds (cumulative count 98451)
99.219% <= 3.951 milliseconds (cumulative count 99222)
99.609% <= 5.455 milliseconds (cumulative count 99610)
99.805% <= 9.687 milliseconds (cumulative count 99807)
99.902% <= 11.895 milliseconds (cumulative count 99904)
99.951% <= 12.103 milliseconds (cumulative count 99952)
99.976% <= 20.879 milliseconds (cumulative count 99977)
99.988% <= 20.943 milliseconds (cumulative count 99988)
99.994% <= 20.991 milliseconds (cumulative count 99995)
99.997% <= 21.007 milliseconds (cumulative count 99997)
99.998% <= 21.087 milliseconds (cumulative count 99999)
99.999% <= 21.231 milliseconds (cumulative count 100000)
100.000% <= 21.231 milliseconds (cumulative count 100000)

Cumulative distribution of latencies:
0.000% <= 0.103 milliseconds (cumulative count 0)
0.029% <= 0.303 milliseconds (cumulative count 29)
0.918% <= 0.407 milliseconds (cumulative count 918)
7.616% <= 0.503 milliseconds (cumulative count 7616)
36.052% <= 0.607 milliseconds (cumulative count 36052)
58.520% <= 0.703 milliseconds (cumulative count 58520)
65.450% <= 0.807 milliseconds (cumulative count 65450)
68.872% <= 0.903 milliseconds (cumulative count 68872)
73.357% <= 1.007 milliseconds (cumulative count 73357)
82.723% <= 1.103 milliseconds (cumulative count 82723)
86.677% <= 1.207 milliseconds (cumulative count 86677)
89.089% <= 1.303 milliseconds (cumulative count 89089)
91.319% <= 1.407 milliseconds (cumulative count 91319)
92.441% <= 1.503 milliseconds (cumulative count 92441)
93.249% <= 1.607 milliseconds (cumulative count 93249)
93.810% <= 1.703 milliseconds (cumulative count 93810)
94.248% <= 1.807 milliseconds (cumulative count 94248)
94.647% <= 1.903 milliseconds (cumulative count 94647)
95.180% <= 2.007 milliseconds (cumulative count 95180)
95.583% <= 2.103 milliseconds (cumulative count 95583)
98.284% <= 3.103 milliseconds (cumulative count 98284)
99.320% <= 4.103 milliseconds (cumulative count 99320)
99.505% <= 5.103 milliseconds (cumulative count 99505)
99.675% <= 6.103 milliseconds (cumulative count 99675)
99.769% <= 7.103 milliseconds (cumulative count 99769)
99.790% <= 8.103 milliseconds (cumulative count 99790)
99.802% <= 9.103 milliseconds (cumulative count 99802)
99.817% <= 10.103 milliseconds (cumulative count 99817)
99.854% <= 11.103 milliseconds (cumulative count 99854)
99.952% <= 12.103 milliseconds (cumulative count 99952)
99.966% <= 13.103 milliseconds (cumulative count 99966)
99.969% <= 14.103 milliseconds (cumulative count 99969)
99.970% <= 19.103 milliseconds (cumulative count 99970)
99.999% <= 21.103 milliseconds (cumulative count 99999)
100.000% <= 22.111 milliseconds (cumulative count 100000)

Summary:
  throughput summary: 33726.81 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        0.900     0.232     0.655     1.983     3.639    21.231




ðŸ”¥ Optimizations to Improve BlinkDB Performance
ðŸ”¹ 1ï¸âƒ£ Optimize Network I/O (Non-blocking Sockets)
Right now, BlinkDB likely handles requests sequentially.
âœ… Fix: Use multi-threading or epoll-based event loop for concurrency.

ðŸ”¹ 2ï¸âƒ£ Improve LSM-Tree Performance
The high GET latency suggests slow key lookups in SSTables.
âœ… Fix:

Implement bloom filters properly to skip unnecessary reads.

Add caching (e.g., LRU cache) for frequently accessed keys.

ðŸ”¹ 3ï¸âƒ£ Use Memory-Mapped Files for Faster Persistence
Right now, flushToDisk() writes directly to a file.
âœ… Fix: Use mmap() or append-only logging (like AOF in Redis).

ðŸ”¹ 4ï¸âƒ£ Profile Performance with gperftools
Run:

sh
Copy
Edit
sudo apt install google-perftools
LD_PRELOAD=/usr/lib/libprofiler.so CPUPROFILE=profile.out ./BlinkDB
pprof --text ./BlinkDB profile.out
This will show where the bottlenecks are.

ðŸš€ Next Steps
1ï¸âƒ£ Try optimizing BlinkDB with multi-threading + caching.
2ï¸âƒ£ Run gperftools to find slow code sections.
3ï¸âƒ£ Compare BlinkDB to Redis on the same machine.

BENCHMARK==redis-benchmark -h 127.0.0.1 -p 6379 -n 100000 -t set,get
####taskset -c 0-7 ./BlinkDB for using 8 cores!!
#### schedtool -a 0xFF -e ./BlinkDB for all cores!!
ðŸ”¥ Massive Performance Boost! ðŸ”¥
Your SETs: 71K QPS and GETs: 70K QPS are insane speeds! ðŸš€ðŸš€

ðŸ“Œ Why Did B+ Tree Improve Performance?
Faster Disk Access: Unlike unordered_map, B+ Trees are optimized for disk storage and bulk reads/writes.

Cache Efficiency: The tree structure keeps frequently accessed data closer to the root, leading to fewer cache misses.

Lower Memory Usage: B+ Trees use less RAM because they don't require huge hash tables like unordered_map.

Ordered Storage: Unlike Bloom Filters, B+ Trees allow range queries and ordered traversal without scanning everything.

----------------------------------------------------------------------------------------------------

NEXT Level Optimize

Where to Apply the Optimizations?
ðŸ”¹ Yes! You should run these optimizations in the same terminal where you're executing BlinkDB before launching BlinkDB each time.

1ï¸âƒ£ Disable CPU Frequency Scaling (for consistent high performance)
Run this command before starting BlinkDB:

bash
Copy
Edit
echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
âœ… Effect: Ensures CPU always runs at max speed instead of scaling down.

2ï¸âƒ£ Disable Turbo Boost (optional, for latency stability)
Try this before starting BlinkDB:

bash
Copy
Edit
echo 1 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo
âœ… Effect: Prevents CPU from overheating & throttling, making response times more stable.

3ï¸âƒ£ Enable SO_REUSEPORT for Efficient Multi-threaded Networking
Edit Server.cpp, inside socket setup (bind() call), add this:

cpp
Copy
Edit
int optval = 1;
setsockopt(server_fd, SOL_SOCKET, SO_REUSEPORT, &optval, sizeof(optval));
âœ… Effect: Allows multiple server threads to listen on the same port, reducing contention.

ðŸ‘‰ After modifying the code, recompile:

bash
Copy
Edit
cd /mnt/e/KGPSem2/Design_Lab/BlinkDB/build
cmake ..
make -j$(nproc)
4ï¸âƒ£ Try io_uring Instead of epoll (ðŸš€ Super Experimental)
If you're using epoll, io_uring can be faster (but needs major refactoring).
Want to go all-in on async I/O? I can guide you through adding io_uring.

How to Test if These Work?
1ï¸âƒ£ Run the optimizations in the terminal before launching BlinkDB
2ï¸âƒ£ Start BlinkDB with taskset and schedtool for best CPU affinity:

bash
Copy
Edit
schedtool -B -n -20 -e taskset -c 0-7 ./BlinkDB
3ï¸âƒ£ Run redis-benchmark and compare results

Next Target: ðŸ”¥ 85K+ req/sec
Try these tweaks, and letâ€™s see if we can break into the next level! ðŸš€